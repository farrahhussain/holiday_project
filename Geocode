## install packages
!pip install webdriver_manager

## Import these
import requests
import pandas as pd
from config import api_key, gkey, weather_key
from pprint import pprint
import json
#from citipy import citipy
'''
##Optional
import gmaps
gmaps.configure(api_key= gkey)
'''
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager
import time


## Webscraping website code
## output of this code is list of activities and their descriptions

# A function to scrape the website headers for information
def web_scraper_headers(url, index, class_):
    # Initialize the Selenium WebDriver (this example uses Chrome)
    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))

    # Fetch the webpage
    driver.get(url)

    # Wait for the page to load completely (adjust the sleep time if necessary)
    time.sleep(5)  # This wait time might need adjustment

    # Get the page source after JavaScript has rendered the content
    web_content = driver.page_source

    # Close the driver
    driver.quit()

    # Parse the HTML content using BeautifulSoup
    soup = BeautifulSoup(web_content, 'html.parser')

    # Find headers
    headers = soup.find_all(index, class_)


    # Extract the text from these headers and store them in a list
    header_texts = [header.get_text() for header in headers]

    return header_texts

# Another function to scrape website descriptions for information
def web_scraper_headers(url, index, class_):
    # Initialize the Selenium WebDriver (this example uses Chrome)
    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))

    # Fetch the webpage
    driver.get(url)

    # Wait for the page to load completely (adjust the sleep time if necessary)
    time.sleep(5)  # This wait time might need adjustment

    # Get the page source after JavaScript has rendered the content
    web_content = driver.page_source

    # Close the driver
    driver.quit()

    # Parse the HTML content using BeautifulSoup
    soup = BeautifulSoup(web_content, 'html.parser')

    # Find headers
    headers = soup.find_all(index, class_)


    # Extract the text from these headers and store them in a list
    header_texts = [header.get_text() for header in headers]

    return header_texts


## Adventure spots
url = 'https://www.skyscanner.net/news/adventures-in-malta'
index = 'h2'
class_ = 'wp-block-heading'

activities = web_scraper_headers(url, index, class_)
activities = activities[:10]


# Cultural spots
url = 'https://www.malta.com/en/attraction/culture/top-10-cultural-sites-in-malta-and-gozo'
index = 'div'
class_ = 'title_arrow_div'

cultural_sites = web_scraper_headers(url, index, class_)

## Relaxing spots
# Beaches
url = 'https://www.houseandgarden.co.uk/article/best-beaches-in-malta'
index = 'div'
class_='heading-h3'

beaches = web_scraper_headers(url, index, class_)

# Spa
url = 'https://www.malta.com/en/attraction/spa-wellness/top-10-spas-in-malta-and-gozo'
index = 'div'
class_ = 'title_arrow_div'

spa = web_scraper_headers(url, index, class_)
spa = spa[:5]

relaxing = beaches + spa

## Nightlife
url = 'https://traveltriangle.com/blog/malta-nightlife/'
index = 'h3'

nightlife_descriptions = web_scraper_descriptions(url, index)

# Cleaning the list
nightlife_descriptions.pop(1)
nightlife_descriptions = nightlife_descriptions[:10]

party = []
for place in nightlife_descriptions:
    party.append(place[3:])


### THE LISTS YOU NEED TO IMPORT INTO GOOGLE GEO API
# ADVENTURE 
activities

# CULTURE
cultural_sites = web_scraper_headers(url, index, class_)

# RELAX
relaxing = beaches + spa

# PARTY
party
